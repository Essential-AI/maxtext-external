base_emb_dim: 2048
base_num_query_heads: 16
base_num_kv_heads: 16
base_mlp_dim: 1408
base_num_decoder_layers: 27
num_dense_layers: 1
expert_affinity_function: "sigmoid"
load_balancing_mode: "loss" # XXX should be bias
q_lora_rank: 0
kv_lora_rank: 512
qk_nope_head_dim: 128
qk_rope_head_dim: 128 # should be 64, but changed to enable flash attention
head_dim: 256 # v_head_dim, should be 128 but changed to enable flash attention
mlp_activations: ["silu","linear"]
vocab_size: 102400
enable_dropout: False
logits_via_embedding: False
normalization_layer_epsilon: 1.0e-6
num_experts: 64
num_experts_per_tok: 6
num_shared_experts: 2
rope_max_timescale: 10_000
decoder_block: "deepseek"
scan_layers: False # don't support scan_layers=True with num_dense_layers > 0
sharding_tolerance: 0.05

# XXX
# mscale = 0.707 is in original config, not respected here
# route_scale?
# rope_factor is not respected either
# should be 2 shared experts, huh
